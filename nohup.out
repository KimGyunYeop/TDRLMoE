wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_134745-gemd84h6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/gemd84h6
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
SwitchTransformersForConditionalGeneration(
  (shared): Embedding(32128, 768)
  (encoder): SwitchTransformersStack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): SwitchTransformersLayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): SwitchTransformersStack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersDenseActDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): SwitchTransformersBlock(
        (layer): ModuleList(
          (0): SwitchTransformersLayerSelfAttention(
            (SelfAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SwitchTransformersLayerCrossAttention(
            (EncDecAttention): SwitchTransformersAttention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SwitchTransformersLayerFF(
            (mlp): SwitchTransformersSparseMLP(
              (router): SwitchTransformersTop1Router(
                (classifier): Linear(in_features=768, out_features=16, bias=False)
              )
              (experts): ModuleDict(
                (expert_0): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_1): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_2): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_3): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_4): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_5): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_6): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_7): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_8): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_9): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_10): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_11): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_12): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_13): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_14): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
                (expert_15): SwitchTransformersDenseActDense(
                  (wi): Linear(in_features=768, out_features=3072, bias=False)
                  (wo): Linear(in_features=3072, out_features=768, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (act): ReLU()
                )
              )
            )
            (layer_norm): SwitchTransformersLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): SwitchTransformersLayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=768, out_features=32128, bias=False)
)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/gemd84h6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_134745-gemd84h6/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_135008-cl4bjuui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/cl4bjuui
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 190, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 87, in main
    rouge_metric = evaluate.load("rouge")
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/loading.py", line 748, in load
    evaluation_module = evaluation_module_factory(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/loading.py", line 639, in evaluation_module_factory
    ).get_module()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/loading.py", line 479, in get_module
    local_path = self.download_loading_script(revision)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/loading.py", line 469, in download_loading_script
    return cached_path(file_path, download_config=download_config)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/utils/file_utils.py", line 175, in cached_path
    output_path = get_from_cache(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/utils/file_utils.py", line 457, in get_from_cache
    response = http_head(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/utils/file_utils.py", line 378, in http_head
    response = _request_with_retry(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/evaluate/utils/file_utils.py", line 307, in _request_with_retry
    response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 1375, in getresponse
    response.begin()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/ssl.py", line 1307, in recv_into
    return self.read(nbytes, buffer)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/ssl.py", line 1163, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7f56e7e996c0>
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 176, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 190, in _atexit_teardown
    self._teardown(self._hooks.exit_code if self._hooks else 0)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 171, in _teardown
    return self._service.join()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/wandb/sdk/service/service.py", line 263, in join
    ret = self._internal_proc.wait()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt: 
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/cl4bjuui[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_135008-cl4bjuui/logs[0m
