/home/nlplab/anaconda3/envs/gyop/bin/python: /home/nlplab/anaconda3/envs/gyop/bin/python: cannot execute binary file
/home/nlplab/anaconda3/envs/gyop/bin/python3: /home/nlplab/anaconda3/envs/gyop/bin/python3: cannot execute binary file
/home/nlplab/anaconda3/envs/gyop/bin/python: /home/nlplab/anaconda3/envs/gyop/bin/python: cannot execute binary file
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_013429-263gxa2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-32
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-32/runs/263gxa2o
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Map:   0%|          | 0/14732 [00:00<?, ? examples/s]/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map:  14%|‚ñà‚ñé        | 2000/14732 [00:00<00:01, 11402.69 examples/s]Map:  27%|‚ñà‚ñà‚ñã       | 4000/14732 [00:00<00:01, 8318.37 examples/s] Map:  41%|‚ñà‚ñà‚ñà‚ñà      | 6000/14732 [00:00<00:00, 9934.78 examples/s]Map:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 8000/14732 [00:00<00:00, 10881.13 examples/s]Map:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 10000/14732 [00:00<00:00, 11403.30 examples/s]Map:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 12000/14732 [00:01<00:00, 11792.80 examples/s]Map:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 14000/14732 [00:01<00:00, 12061.59 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14732/14732 [00:01<00:00, 11101.83 examples/s]
Map:   0%|          | 0/819 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [00:00<00:00, 11972.41 examples/s]
Map:   0%|          | 0/818 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 818/818 [00:00<00:00, 12271.33 examples/s]
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:139: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-32",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 32,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0.01,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "torch_dtype": "float32",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

  0%|          | 0/9210 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 187, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 152, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 512.00 KiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.74 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-32/runs/263gxa2o[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_013429-263gxa2o/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_015538-0g5sudol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-32
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-32/runs/0g5sudol
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Map:   0%|          | 0/819 [00:00<?, ? examples/s]/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [00:00<00:00, 9868.50 examples/s]
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-32",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 32,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0.01,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "torch_dtype": "float32",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

  0%|          | 0/2300 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 512.00 KiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-32/runs/0g5sudol[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_015538-0g5sudol/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_015620-svr43vso
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/svr43vso
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Map:   0%|          | 0/819 [00:00<?, ? examples/s]/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [00:00<00:00, 9427.73 examples/s]
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

  0%|          | 0/2300 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/2300 [00:11<7:33:13, 11.83s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 2.50 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/svr43vso[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_015620-svr43vso/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_023250-wb2ikt1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/wb2ikt1w
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Map:   0%|          | 0/818 [00:00<?, ? examples/s]/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 818/818 [00:00<00:00, 10216.03 examples/s]
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

  0%|          | 0/2300 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/2300 [00:12<7:48:00, 12.21s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 2.50 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/wb2ikt1w[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_023250-wb2ikt1w/logs[0m
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
E0221 02:49:41.699000 140532635447680 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 2) local_rank: 0 (pid: 2600590) of binary: /home/nlplab/anaconda3/envs/gyop/bin/python
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
python FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-21_02:49:41
  host      : nlplab7
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 2600591)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-21_02:49:41
  host      : nlplab7
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 2600592)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-21_02:49:41
  host      : nlplab7
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 2600593)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-21_02:49:41
  host      : nlplab7
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 2600590)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
/home/nlplab/anaconda3/envs/gyop/bin/python: can't open file '/home/nlplab/ssd1/gyop/research/TDRLMoE/python': [Errno 2] No such file or directory
E0221 02:51:48.784000 139716574048640 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 2) local_rank: 0 (pid: 2602343) of binary: /home/nlplab/anaconda3/envs/gyop/bin/python
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
python FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-21_02:51:48
  host      : nlplab7
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 2602344)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-21_02:51:48
  host      : nlplab7
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 2602345)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-21_02:51:48
  host      : nlplab7
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 2602346)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-21_02:51:48
  host      : nlplab7
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 2602343)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025159-eo8u5hq7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/eo8u5hq7
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025159-vovn2b15
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/vovn2b15
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025159-h3l8tqnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/h3l8tqnp
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025159-pn7j6g2a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/pn7j6g2a
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
[2025-02-21 02:52:17,162] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-21 02:52:17,169] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-21 02:52:17,252] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-21 02:52:17,322] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: df: /home/nlplab/.triton/autotune/home/nlplab/.triton/autotune: No such file or directory: No such file or directory

  0%|          | 0/2300 [00:00<?, ?it/s][rank3]: Traceback (most recent call last):
[rank3]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
[rank3]:     main()
[rank3]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
[rank3]:     trainer.train()
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank3]:     return model_forward(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank3]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1729, in forward
[rank3]:     encoder_outputs = self.encoder(
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1052, in forward
[rank3]:     layer_outputs = layer_module(
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 742, in forward
[rank3]:     hidden_states = self.layer[-1](hidden_states, output_router_logits)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 344, in forward
[rank3]:     forwarded_states = self.mlp(forwarded_states)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 309, in forward
[rank3]:     next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
[rank3]: RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
[rank1]:     main()
[rank1]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
[rank1]:     trainer.train()
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1729, in forward
[rank1]:     encoder_outputs = self.encoder(
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1052, in forward
[rank1]:     layer_outputs = layer_module(
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 742, in forward
[rank1]:     hidden_states = self.layer[-1](hidden_states, output_router_logits)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 344, in forward
[rank1]:     forwarded_states = self.mlp(forwarded_states)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 309, in forward
[rank1]:     next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
[rank1]: RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
[rank0]:     main()
[rank0]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1729, in forward
[rank0]:     encoder_outputs = self.encoder(
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1052, in forward
[rank0]:     layer_outputs = layer_module(
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 742, in forward
[rank0]:     hidden_states = self.layer[-1](hidden_states, output_router_logits)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 344, in forward
[rank0]:     forwarded_states = self.mlp(forwarded_states)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 309, in forward
[rank0]:     next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
[rank0]: RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
[rank2]:     main()
[rank2]:   File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
[rank2]:     trainer.train()
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank2]:     return model_forward(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank2]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1729, in forward
[rank2]:     encoder_outputs = self.encoder(
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1052, in forward
[rank2]:     layer_outputs = layer_module(
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 742, in forward
[rank2]:     hidden_states = self.layer[-1](hidden_states, output_router_logits)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 344, in forward
[rank2]:     forwarded_states = self.mlp(forwarded_states)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 309, in forward
[rank2]:     next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
[rank2]: RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/vovn2b15[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025159-vovn2b15/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/eo8u5hq7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025159-eo8u5hq7/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/h3l8tqnp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025159-h3l8tqnp/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/pn7j6g2a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025159-pn7j6g2a/logs[0m
W0221 02:52:25.093000 140598098579840 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2602547 closing signal SIGTERM
W0221 02:52:25.094000 140598098579840 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2602548 closing signal SIGTERM
W0221 02:52:25.094000 140598098579840 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2602549 closing signal SIGTERM
E0221 02:52:25.658000 140598098579840 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2602546) of binary: /home/nlplab/anaconda3/envs/gyop/bin/python
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
baseline_switchT.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-21_02:52:25
  host      : nlplab7
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2602546)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025437-x52cbabc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/x52cbabc
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 02:54:47,334] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/2300 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/2300 [00:11<7:35:08, 11.88s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 2.50 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/x52cbabc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025437-x52cbabc/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_025614-2itxng8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/2itxng8l
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 02:56:25,042] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/9210 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/9210 [00:07<18:02:21,  7.05s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 18.50 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/2itxng8l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_025614-2itxng8l/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_030421-z0fvvd2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/z0fvvd2u
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:04:33,438] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/9210 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/9210 [00:06<17:38:20,  6.90s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 18.50 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/z0fvvd2u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_030421-z0fvvd2u/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_030753-j756xw0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/j756xw0r
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:08:11,186] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/36830 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 192, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 157, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1729, in forward
    encoder_outputs = self.encoder(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1052, in forward
    layer_outputs = layer_module(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 742, in forward
    hidden_states = self.layer[-1](hidden_states, output_router_logits)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 344, in forward
    forwarded_states = self.mlp(forwarded_states)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 309, in forward
    next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/j756xw0r[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_030753-j756xw0r/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_030946-i69opxve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/i69opxve
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:10:08,775] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/36830 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 158, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1789, in forward
    encoder_router_logits, encoder_expert_indexes = self._unpack_router_logits(encoder_outputs[-1])
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1851, in _unpack_router_logits
    return torch.cat(total_router_logits, dim=1), torch.cat(total_expert_indexes, dim=1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/i69opxve[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_030946-i69opxve/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_031546-koth2183
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/koth2183
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 214, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 78, in main
    modeling_switch_transformers.SwitchTransformerLayer._unpack_router_logits = patched_unpack_router_logits
AttributeError: module 'transformers.models.switch_transformers.modeling_switch_transformers' has no attribute 'SwitchTransformerLayer'
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/koth2183[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_031546-koth2183/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_031617-ccrprwe4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/ccrprwe4
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:167: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:16:35,115] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/36830 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 215, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 180, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/models/switch_transformers/modeling_switch_transformers.py", line 1789, in forward
    encoder_router_logits, encoder_expert_indexes = self._unpack_router_logits(encoder_outputs[-1])
TypeError: main.<locals>.patched_unpack_router_logits() missing 1 required positional argument: 'router_outputs'
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/ccrprwe4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_031617-ccrprwe4/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_031806-c6aspe5d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/c6aspe5d
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:18:25,049] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/9210 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/9210 [00:06<17:33:19,  6.86s/it]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 158, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 2192, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 306, in apply
    return user_fn(self, *args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 142, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 94, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 14.50 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/c6aspe5d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_031806-c6aspe5d/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032332-tobuklfc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/tobuklfc
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:146: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:23:48,356] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 194, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 159, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2269, in _inner_training_loop
    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 398, in deepspeed_init
    hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 270, in trainer_config_finalize
    raise ValueError(
ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
- ds scheduler.params.warmup_num_steps=500 vs hf warmup_steps=0
The easiest method is to set these DeepSpeed config values to 'auto'.
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/tobuklfc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032332-tobuklfc/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032453-lud4eev9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/lud4eev9
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:146: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:25:09,478] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/9210 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 194, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 159, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 172, in forward
    raise RuntimeError("module must have its parameters and buffers "
RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/lud4eev9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032453-lud4eev9/logs[0m
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032606-c1lbxnxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/c1lbxnxh
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032606-m2tf6kd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/m2tf6kd6
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032606-cpzhvb5d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/cpzhvb5d
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032606-fo72u914
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/fo72u914
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 63, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, dtype=torch.float32)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4111, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: SwitchTransformersForConditionalGeneration.__init__() got an unexpected keyword argument 'dtype'
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 63, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, dtype=torch.float32)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4111, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: SwitchTransformersForConditionalGeneration.__init__() got an unexpected keyword argument 'dtype'
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 63, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, dtype=torch.float32)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4111, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: SwitchTransformersForConditionalGeneration.__init__() got an unexpected keyword argument 'dtype'
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 193, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 63, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, dtype=torch.float32)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4111, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: SwitchTransformersForConditionalGeneration.__init__() got an unexpected keyword argument 'dtype'
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/cpzhvb5d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032606-cpzhvb5d/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/fo72u914[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032606-fo72u914/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/m2tf6kd6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032606-m2tf6kd6/logs[0m
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/c1lbxnxh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032606-c1lbxnxh/logs[0m
W0221 03:26:24.149000 140522822107520 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2632333 closing signal SIGTERM
W0221 03:26:24.149000 140522822107520 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2632334 closing signal SIGTERM
W0221 03:26:24.149000 140522822107520 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2632336 closing signal SIGTERM
E0221 03:26:24.213000 140522822107520 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2632335) of binary: /home/nlplab/anaconda3/envs/gyop/bin/python
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
baseline_switchT.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-21_03:26:24
  host      : nlplab7
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2632335)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/home/nlplab/anaconda3/envs/gyop/bin/accelerate", line 5, in <module>
    from accelerate.commands.accelerate_cli import main
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/__init__.py", line 16, in <module>
    from .accelerator import Accelerator
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py", line 32, in <module>
    import torch
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/__init__.py", line 2143, in <module>
    from . import _meta_registrations
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_meta_registrations.py", line 6178, in <module>
    activate_meta()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/_meta_registrations.py", line 6175, in activate_meta
    _meta_lib_dont_use_me_use_register_meta.impl(op_overload, fn)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/library.py", line 255, in impl
    self.m.impl(name, dispatch_key if dispatch_key != "" else "CompositeImplicitAutograd", fn, with_keyset)
KeyboardInterrupt
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_032916-h3r2xr9z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/h3r2xr9z
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 214, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 84, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, device_map=device_map)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4194, in from_pretrained
    check_tied_parameters_on_same_device(tied_params, device_map)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 611, in check_tied_parameters_on_same_device
    tie_param_devices[param] = _get_param_device(param, device_map)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 593, in _get_param_device
    return _get_param_device(parent_param, device_map)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 593, in _get_param_device
    return _get_param_device(parent_param, device_map)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 591, in _get_param_device
    raise ValueError(f"The `device_map` does not contain the module {param}.")
ValueError: The `device_map` does not contain the module .
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/h3r2xr9z[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_032916-h3r2xr9z/logs[0m
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 171, in <module>
    main()
NameError: name 'main' is not defined. Did you mean: 'min'?
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_033254-94p0l5li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/94p0l5li
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 187, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 60, in main
    model = SwitchTransformersForConditionalGeneration.from_pretrained(args.model_name, device_map)
NameError: name 'device_map' is not defined
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/94p0l5li[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_033254-94p0l5li/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_033317-wrkljryd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/wrkljryd
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 187, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 53, in main
    dataset = load_dataset(args.dataset_name)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/load.py", line 1638, in dataset_module_factory
    ).get_module()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/load.py", line 1295, in get_module
    dataset_readme_path = self.download_dataset_readme_file()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/load.py", line 1276, in download_dataset_readme_file
    return cached_path(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/datasets/utils/file_utils.py", line 182, in cached_path
    output_path = huggingface_hub.HfApi(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 5548, in hf_hub_download
    return hf_hub_download(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1295, in _hf_hub_download_to_cache_dir
    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1746, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1666, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 383, in _request_wrapper
    return _request_wrapper(method=method, url=next_url, follow_relative_redirects=True, **params)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 364, in _request_wrapper
    response = _request_wrapper(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 387, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 93, in send
    return super().send(request, *args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connectionpool.py", line 536, in _make_request
    response = conn.getresponse()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/urllib3/connection.py", line 507, in getresponse
    httplib_response = super().getresponse()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 1375, in getresponse
    response.begin()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/ssl.py", line 1307, in recv_into
    return self.read(nbytes, buffer)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/ssl.py", line 1163, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/wrkljryd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_033317-wrkljryd/logs[0m
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: gyunyeop (isnlp_lab). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /home/nlplab/ssd1/gyop/research/TDRLMoE/wandb/run-20250221_033334-z2a6urkw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run switch-samsum-base16-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16
wandb: üöÄ View run at https://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/z2a6urkw
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py:139: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
DatasetDict({
    train: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 14732
    })
    test: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 819
    })
    validation: Dataset({
        features: ['id', 'dialogue', 'summary'],
        num_rows: 818
    })
})
tokenizer token map {'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}
model config: SwitchTransformersConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "google/switch-base-16",
  "add_router_probs": false,
  "architectures": [
    "SwitchTransformersForConditionalGeneration"
  ],
  "batch_prioritized_routing": false,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_sparse_step": 2,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "encoder_sparse_step": 2,
  "eos_token_id": 1,
  "expert_capacity": 64,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "switch_transformers",
  "num_decoder_layers": 12,
  "num_experts": 16,
  "num_heads": 12,
  "num_layers": 12,
  "num_selected_experts": 1,
  "num_sparse_decoder_layers": 6,
  "num_sparse_encoder_layers": 6,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "router_aux_loss_coef": 0.001,
  "router_bias": false,
  "router_dtype": "float32",
  "router_ignore_padding_tokens": false,
  "router_jitter_noise": 0,
  "router_type": "tokens_masked",
  "router_z_loss_coef": 0.001,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2025-02-21 03:33:54,994] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/18420 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/18420 [00:04<23:54:12,  4.67s/it]  0%|          | 2/18420 [00:07<16:52:19,  3.30s/it]  0%|          | 3/18420 [00:09<15:01:16,  2.94s/it]  0%|          | 4/18420 [00:11<13:56:57,  2.73s/it]  0%|          | 5/18420 [00:14<13:19:39,  2.61s/it]  0%|          | 6/18420 [00:16<12:57:39,  2.53s/it]  0%|          | 7/18420 [00:19<12:46:06,  2.50s/it]  0%|          | 8/18420 [00:21<12:37:35,  2.47s/it]  0%|          | 9/18420 [00:23<12:29:07,  2.44s/it]  0%|          | 10/18420 [00:26<12:28:58,  2.44s/it]  0%|          | 11/18420 [00:28<12:28:26,  2.44s/it]  0%|          | 12/18420 [00:31<12:25:30,  2.43s/it]  0%|          | 13/18420 [00:33<12:21:20,  2.42s/it]  0%|          | 14/18420 [00:35<12:18:47,  2.41s/it]  0%|          | 15/18420 [00:38<12:17:56,  2.41s/it]  0%|          | 16/18420 [00:40<12:17:09,  2.40s/it]  0%|          | 17/18420 [00:43<12:12:51,  2.39s/it]  0%|          | 18/18420 [00:45<12:12:37,  2.39s/it]  0%|          | 19/18420 [00:47<12:14:59,  2.40s/it]  0%|          | 20/18420 [00:50<12:16:38,  2.40s/it]  0%|          | 21/18420 [00:52<12:17:08,  2.40s/it]  0%|          | 22/18420 [00:55<12:16:30,  2.40s/it]  0%|          | 23/18420 [00:57<12:18:01,  2.41s/it]  0%|          | 24/18420 [00:59<12:17:37,  2.41s/it]  0%|          | 25/18420 [01:02<12:33:54,  2.46s/it]  0%|          | 26/18420 [01:04<12:26:45,  2.44s/it]  0%|          | 27/18420 [01:07<12:22:05,  2.42s/it]  0%|          | 28/18420 [01:09<12:19:11,  2.41s/it]  0%|          | 29/18420 [01:12<12:17:00,  2.40s/it]  0%|          | 30/18420 [01:14<12:15:05,  2.40s/it]  0%|          | 31/18420 [01:16<12:14:09,  2.40s/it]  0%|          | 32/18420 [01:19<12:12:35,  2.39s/it]  0%|          | 33/18420 [01:21<12:10:26,  2.38s/it]  0%|          | 34/18420 [01:24<12:10:15,  2.38s/it]  0%|          | 35/18420 [01:26<12:10:48,  2.39s/it]  0%|          | 36/18420 [01:28<12:09:25,  2.38s/it]  0%|          | 37/18420 [01:31<12:11:54,  2.39s/it]  0%|          | 38/18420 [01:33<12:27:21,  2.44s/it]  0%|          | 39/18420 [01:36<12:23:44,  2.43s/it]  0%|          | 40/18420 [01:38<12:22:34,  2.42s/it]  0%|          | 41/18420 [01:40<12:16:46,  2.41s/it]  0%|          | 42/18420 [01:43<12:17:09,  2.41s/it]  0%|          | 43/18420 [01:45<12:19:07,  2.41s/it]  0%|          | 44/18420 [01:48<12:18:21,  2.41s/it]  0%|          | 45/18420 [01:50<12:16:23,  2.40s/it]  0%|          | 46/18420 [01:52<12:21:16,  2.42s/it]  0%|          | 47/18420 [01:55<12:19:10,  2.41s/it]  0%|          | 48/18420 [01:57<12:16:10,  2.40s/it]  0%|          | 49/18420 [02:00<12:13:04,  2.39s/it]  0%|          | 50/18420 [02:02<12:13:43,  2.40s/it]  0%|          | 51/18420 [02:04<12:15:42,  2.40s/it]  0%|          | 52/18420 [02:07<12:15:29,  2.40s/it]  0%|          | 53/18420 [02:09<12:15:25,  2.40s/it]  0%|          | 54/18420 [02:12<12:12:44,  2.39s/it]  0%|          | 55/18420 [02:14<12:14:52,  2.40s/it]  0%|          | 56/18420 [02:16<12:15:46,  2.40s/it]  0%|          | 57/18420 [02:19<12:12:48,  2.39s/it]  0%|          | 58/18420 [02:21<12:15:13,  2.40s/it]  0%|          | 59/18420 [02:24<12:17:29,  2.41s/it]  0%|          | 60/18420 [02:26<12:15:40,  2.40s/it]  0%|          | 61/18420 [02:28<12:12:40,  2.39s/it]  0%|          | 62/18420 [02:31<12:12:23,  2.39s/it]  0%|          | 63/18420 [02:33<12:14:44,  2.40s/it]  0%|          | 64/18420 [02:36<12:15:22,  2.40s/it]  0%|          | 65/18420 [02:38<12:12:15,  2.39s/it]  0%|          | 66/18420 [02:40<12:13:12,  2.40s/it]  0%|          | 67/18420 [02:43<12:13:32,  2.40s/it]  0%|          | 68/18420 [02:45<12:13:29,  2.40s/it]  0%|          | 69/18420 [02:48<12:13:36,  2.40s/it]  0%|          | 70/18420 [02:50<12:11:47,  2.39s/it]  0%|          | 71/18420 [02:52<12:12:30,  2.40s/it]  0%|          | 72/18420 [02:55<12:28:28,  2.45s/it]  0%|          | 73/18420 [02:57<12:21:58,  2.43s/it]  0%|          | 74/18420 [03:00<12:20:41,  2.42s/it]  0%|          | 75/18420 [03:02<12:19:26,  2.42s/it]  0%|          | 76/18420 [03:05<12:16:55,  2.41s/it]  0%|          | 77/18420 [03:07<12:13:48,  2.40s/it]  0%|          | 78/18420 [03:09<12:09:13,  2.39s/it]  0%|          | 79/18420 [03:12<12:09:27,  2.39s/it]  0%|          | 80/18420 [03:14<12:09:54,  2.39s/it]  0%|          | 81/18420 [03:16<12:08:59,  2.39s/it]  0%|          | 82/18420 [03:19<12:11:51,  2.39s/it]  0%|          | 83/18420 [03:21<12:14:37,  2.40s/it]  0%|          | 84/18420 [03:24<12:15:03,  2.41s/it]  0%|          | 85/18420 [03:26<12:13:14,  2.40s/it]  0%|          | 86/18420 [03:29<12:12:04,  2.40s/it]  0%|          | 87/18420 [03:31<12:13:52,  2.40s/it]  0%|          | 88/18420 [03:33<12:13:52,  2.40s/it]  0%|          | 89/18420 [03:36<12:25:40,  2.44s/it]  0%|          | 90/18420 [03:38<12:22:11,  2.43s/it]  0%|          | 91/18420 [03:41<12:21:05,  2.43s/it]  0%|          | 92/18420 [03:43<12:17:13,  2.41s/it]  1%|          | 93/18420 [03:45<12:14:37,  2.41s/it]  1%|          | 94/18420 [03:48<12:14:57,  2.41s/it]  1%|          | 95/18420 [03:50<12:15:55,  2.41s/it]  1%|          | 96/18420 [03:53<12:16:02,  2.41s/it]  1%|          | 97/18420 [03:55<12:11:18,  2.39s/it]  1%|          | 98/18420 [03:57<12:08:24,  2.39s/it]  1%|          | 99/18420 [04:00<12:11:35,  2.40s/it]  1%|          | 100/18420 [04:02<12:10:25,  2.39s/it]                                                        1%|          | 100/18420 [04:02<12:10:25,  2.39s/it]  1%|          | 101/18420 [04:05<12:08:35,  2.39s/it]  1%|          | 102/18420 [04:07<12:08:45,  2.39s/it]  1%|          | 103/18420 [04:09<12:10:45,  2.39s/it]  1%|          | 104/18420 [04:12<12:11:38,  2.40s/it]  1%|          | 105/18420 [04:14<12:07:48,  2.38s/it]  1%|          | 106/18420 [04:17<12:24:37,  2.44s/it]  1%|          | 107/18420 [04:19<12:23:45,  2.44s/it]  1%|          | 108/18420 [04:22<12:20:32,  2.43s/it]  1%|          | 109/18420 [04:24<12:17:27,  2.42s/it]  1%|          | 110/18420 [04:26<12:15:04,  2.41s/it]  1%|          | 111/18420 [04:29<12:14:39,  2.41s/it]  1%|          | 112/18420 [04:31<12:14:26,  2.41s/it]  1%|          | 113/18420 [04:34<12:11:44,  2.40s/it]  1%|          | 114/18420 [04:36<12:13:06,  2.40s/it]  1%|          | 115/18420 [04:38<12:16:52,  2.42s/it]  1%|          | 116/18420 [04:41<12:15:25,  2.41s/it]  1%|          | 117/18420 [04:43<12:12:49,  2.40s/it]  1%|          | 118/18420 [04:46<12:11:58,  2.40s/it]  1%|          | 119/18420 [04:48<12:11:55,  2.40s/it]  1%|          | 120/18420 [04:50<12:12:30,  2.40s/it]  1%|          | 121/18420 [04:53<12:08:56,  2.39s/it]  1%|          | 122/18420 [04:55<12:09:26,  2.39s/it]  1%|          | 123/18420 [04:58<12:26:37,  2.45s/it]  1%|          | 124/18420 [05:00<12:21:49,  2.43s/it]  1%|          | 125/18420 [05:02<12:17:35,  2.42s/it]  1%|          | 126/18420 [05:05<12:15:51,  2.41s/it]  1%|          | 127/18420 [05:07<12:15:27,  2.41s/it]  1%|          | 128/18420 [05:10<12:14:37,  2.41s/it]  1%|          | 129/18420 [05:12<12:08:11,  2.39s/it]  1%|          | 130/18420 [05:14<12:10:42,  2.40s/it]  1%|          | 131/18420 [05:17<12:13:10,  2.41s/it]  1%|          | 132/18420 [05:19<12:13:23,  2.41s/it]  1%|          | 133/18420 [05:22<12:11:18,  2.40s/it]  1%|          | 134/18420 [05:24<12:12:15,  2.40s/it]  1%|          | 135/18420 [05:26<12:10:51,  2.40s/it]  1%|          | 136/18420 [05:29<12:09:54,  2.40s/it]  1%|          | 137/18420 [05:31<12:04:59,  2.38s/it]  1%|          | 138/18420 [05:34<12:07:38,  2.39s/it]  1%|          | 139/18420 [05:36<12:10:08,  2.40s/it]  1%|          | 140/18420 [05:39<12:27:41,  2.45s/it]  1%|          | 141/18420 [05:41<12:21:56,  2.44s/it]  1%|          | 142/18420 [05:43<12:19:25,  2.43s/it]  1%|          | 143/18420 [05:46<12:18:41,  2.42s/it]  1%|          | 144/18420 [05:48<12:16:24,  2.42s/it]  1%|          | 145/18420 [05:51<12:13:24,  2.41s/it]  1%|          | 146/18420 [05:53<12:11:57,  2.40s/it]  1%|          | 147/18420 [05:55<12:12:40,  2.41s/it]  1%|          | 148/18420 [05:58<12:12:43,  2.41s/it]  1%|          | 149/18420 [06:00<12:11:34,  2.40s/it]  1%|          | 150/18420 [06:03<12:11:48,  2.40s/it]  1%|          | 151/18420 [06:05<12:11:01,  2.40s/it]  1%|          | 152/18420 [06:07<12:11:10,  2.40s/it]  1%|          | 153/18420 [06:10<12:07:35,  2.39s/it]  1%|          | 154/18420 [06:12<12:05:20,  2.38s/it]  1%|          | 155/18420 [06:15<12:07:48,  2.39s/it]  1%|          | 156/18420 [06:17<12:09:53,  2.40s/it]  1%|          | 157/18420 [06:19<12:07:00,  2.39s/it]  1%|          | 158/18420 [06:22<12:08:43,  2.39s/it]  1%|          | 159/18420 [06:24<12:08:02,  2.39s/it]  1%|          | 160/18420 [06:27<12:08:07,  2.39s/it]  1%|          | 161/18420 [06:29<12:19:41,  2.43s/it]  1%|          | 162/18420 [06:31<12:17:34,  2.42s/it]  1%|          | 163/18420 [06:34<12:17:33,  2.42s/it]  1%|          | 164/18420 [06:36<12:14:54,  2.42s/it]  1%|          | 165/18420 [06:39<12:13:40,  2.41s/it]  1%|          | 166/18420 [06:41<12:12:16,  2.41s/it]  1%|          | 167/18420 [06:43<12:11:11,  2.40s/it]  1%|          | 168/18420 [06:46<12:11:09,  2.40s/it]  1%|          | 169/18420 [06:48<12:07:20,  2.39s/it]  1%|          | 170/18420 [06:51<12:09:02,  2.40s/it]  1%|          | 171/18420 [06:53<12:11:01,  2.40s/it]  1%|          | 172/18420 [06:55<12:11:27,  2.41s/it]  1%|          | 173/18420 [06:58<12:09:12,  2.40s/it]  1%|          | 174/18420 [07:00<12:10:04,  2.40s/it]  1%|          | 175/18420 [07:03<12:10:04,  2.40s/it]  1%|          | 176/18420 [07:05<12:10:42,  2.40s/it]  1%|          | 177/18420 [07:07<12:07:28,  2.39s/it]  1%|          | 178/18420 [07:10<12:08:55,  2.40s/it]  1%|          | 179/18420 [07:12<12:11:00,  2.40s/it]  1%|          | 180/18420 [07:15<12:10:40,  2.40s/it]  1%|          | 181/18420 [07:17<12:07:29,  2.39s/it]  1%|          | 182/18420 [07:20<12:22:59,  2.44s/it]  1%|          | 183/18420 [07:22<12:18:32,  2.43s/it]  1%|          | 184/18420 [07:24<12:17:21,  2.43s/it]  1%|          | 185/18420 [07:27<12:11:53,  2.41s/it]  1%|          | 186/18420 [07:29<12:10:59,  2.41s/it]  1%|          | 187/18420 [07:32<12:10:56,  2.41s/it]  1%|          | 188/18420 [07:34<12:08:15,  2.40s/it]  1%|          | 189/18420 [07:36<12:04:34,  2.38s/it]  1%|          | 190/18420 [07:39<12:03:07,  2.38s/it]  1%|          | 191/18420 [07:41<12:03:34,  2.38s/it]  1%|          | 192/18420 [07:43<12:04:03,  2.38s/it]  1%|          | 193/18420 [07:46<12:01:59,  2.38s/it]  1%|          | 194/18420 [07:48<12:03:41,  2.38s/it]  1%|          | 195/18420 [07:51<12:07:55,  2.40s/it]  1%|          | 196/18420 [07:53<12:06:34,  2.39s/it]  1%|          | 197/18420 [07:55<12:06:00,  2.39s/it]  1%|          | 198/18420 [07:58<12:06:52,  2.39s/it]  1%|          | 199/18420 [08:00<12:24:18,  2.45s/it]  1%|          | 200/18420 [08:03<12:20:46,  2.44s/it]                                                        1%|          | 200/18420 [08:03<12:20:46,  2.44s/it]  1%|          | 201/18420 [08:05<12:14:19,  2.42s/it]  1%|          | 202/18420 [08:08<12:13:55,  2.42s/it]  1%|          | 203/18420 [08:10<12:13:56,  2.42s/it]  1%|          | 204/18420 [08:12<12:11:28,  2.41s/it]  1%|          | 205/18420 [08:15<12:09:15,  2.40s/it]  1%|          | 206/18420 [08:17<12:09:02,  2.40s/it]  1%|          | 207/18420 [08:20<12:08:24,  2.40s/it]  1%|          | 208/18420 [08:22<12:07:04,  2.40s/it]  1%|          | 209/18420 [08:24<12:05:17,  2.39s/it]  1%|          | 210/18420 [08:27<12:06:58,  2.40s/it]  1%|          | 211/18420 [08:29<12:10:20,  2.41s/it]  1%|          | 212/18420 [08:32<12:11:02,  2.41s/it]  1%|          | 213/18420 [08:34<12:09:46,  2.40s/it]  1%|          | 214/18420 [08:36<12:08:38,  2.40s/it]  1%|          | 215/18420 [08:39<12:07:24,  2.40s/it]  1%|          | 216/18420 [08:41<12:05:35,  2.39s/it]  1%|          | 217/18420 [08:43<12:01:10,  2.38s/it]  1%|          | 218/18420 [08:46<12:00:41,  2.38s/it]  1%|          | 219/18420 [08:48<12:02:38,  2.38s/it]  1%|          | 220/18420 [08:51<12:03:27,  2.39s/it]  1%|          | 221/18420 [08:53<12:03:05,  2.38s/it]  1%|          | 222/18420 [08:55<12:04:34,  2.39s/it]  1%|          | 223/18420 [08:58<12:07:58,  2.40s/it]  1%|          | 224/18420 [09:00<12:06:41,  2.40s/it]  1%|          | 225/18420 [09:03<12:02:45,  2.38s/it]  1%|          | 226/18420 [09:05<12:05:46,  2.39s/it]  1%|          | 227/18420 [09:07<12:10:15,  2.41s/it]  1%|          | 228/18420 [09:10<12:11:52,  2.41s/it]  1%|          | 229/18420 [09:12<12:09:59,  2.41s/it]  1%|          | 230/18420 [09:15<12:08:28,  2.40s/it]  1%|‚ñè         | 231/18420 [09:17<12:08:05,  2.40s/it]  1%|‚ñè         | 232/18420 [09:19<12:08:53,  2.40s/it]  1%|‚ñè         | 233/18420 [09:22<12:04:40,  2.39s/it]  1%|‚ñè         | 234/18420 [09:24<12:07:07,  2.40s/it]  1%|‚ñè         | 235/18420 [09:27<12:09:54,  2.41s/it]  1%|‚ñè         | 236/18420 [09:29<12:09:35,  2.41s/it]  1%|‚ñè         | 237/18420 [09:31<12:08:31,  2.40s/it]  1%|‚ñè         | 238/18420 [09:34<12:07:34,  2.40s/it]  1%|‚ñè         | 239/18420 [09:36<12:05:47,  2.40s/it]  1%|‚ñè         | 240/18420 [09:39<12:02:12,  2.38s/it]  1%|‚ñè         | 241/18420 [09:41<11:59:06,  2.37s/it]  1%|‚ñè         | 242/18420 [09:43<12:01:46,  2.38s/it]  1%|‚ñè         | 243/18420 [09:46<12:03:40,  2.39s/it]  1%|‚ñè         | 244/18420 [09:48<12:03:04,  2.39s/it]  1%|‚ñè         | 245/18420 [09:51<12:04:13,  2.39s/it]  1%|‚ñè         | 246/18420 [09:53<12:05:27,  2.40s/it]  1%|‚ñè         | 247/18420 [09:55<12:07:18,  2.40s/it]  1%|‚ñè         | 248/18420 [09:58<12:08:58,  2.41s/it]  1%|‚ñè         | 249/18420 [10:00<12:03:13,  2.39s/it]  1%|‚ñè         | 250/18420 [10:03<12:04:51,  2.39s/it]  1%|‚ñè         | 251/18420 [10:05<12:06:00,  2.40s/it]  1%|‚ñè         | 252/18420 [10:07<12:06:16,  2.40s/it]  1%|‚ñè         | 253/18420 [10:10<12:06:09,  2.40s/it]  1%|‚ñè         | 254/18420 [10:12<12:05:23,  2.40s/it]  1%|‚ñè         | 255/18420 [10:15<12:07:55,  2.40s/it]  1%|‚ñè         | 256/18420 [10:17<12:08:45,  2.41s/it]  1%|‚ñè         | 257/18420 [10:19<12:07:49,  2.40s/it]  1%|‚ñè         | 258/18420 [10:22<12:08:04,  2.41s/it]  1%|‚ñè         | 259/18420 [10:24<12:10:06,  2.41s/it]  1%|‚ñè         | 260/18420 [10:27<12:08:20,  2.41s/it]  1%|‚ñè         | 261/18420 [10:29<12:06:10,  2.40s/it]  1%|‚ñè         | 262/18420 [10:32<12:21:01,  2.45s/it]  1%|‚ñè         | 263/18420 [10:34<12:17:12,  2.44s/it]  1%|‚ñè         | 264/18420 [10:36<12:12:37,  2.42s/it]  1%|‚ñè         | 265/18420 [10:39<12:09:46,  2.41s/it]  1%|‚ñè         | 266/18420 [10:41<12:09:29,  2.41s/it]  1%|‚ñè         | 267/18420 [10:44<12:10:57,  2.42s/it]  1%|‚ñè         | 268/18420 [10:46<12:09:28,  2.41s/it]  1%|‚ñè         | 269/18420 [10:48<12:09:46,  2.41s/it]  1%|‚ñè         | 270/18420 [10:51<12:09:05,  2.41s/it]  1%|‚ñè         | 271/18420 [10:53<12:08:48,  2.41s/it]  1%|‚ñè         | 272/18420 [10:56<12:08:35,  2.41s/it]  1%|‚ñè         | 273/18420 [10:58<12:03:51,  2.39s/it]  1%|‚ñè         | 274/18420 [11:00<12:05:08,  2.40s/it]  1%|‚ñè         | 275/18420 [11:03<12:06:24,  2.40s/it]  1%|‚ñè         | 276/18420 [11:05<12:03:59,  2.39s/it]  2%|‚ñè         | 277/18420 [11:08<12:01:39,  2.39s/it]  2%|‚ñè         | 278/18420 [11:10<12:03:43,  2.39s/it]  2%|‚ñè         | 279/18420 [11:13<12:20:45,  2.45s/it]  2%|‚ñè         | 280/18420 [11:15<12:16:17,  2.44s/it]  2%|‚ñè         | 281/18420 [11:17<12:11:04,  2.42s/it]  2%|‚ñè         | 282/18420 [11:20<12:09:49,  2.41s/it]  2%|‚ñè         | 283/18420 [11:22<12:08:27,  2.41s/it]  2%|‚ñè         | 284/18420 [11:24<12:05:50,  2.40s/it]  2%|‚ñè         | 285/18420 [11:27<12:04:30,  2.40s/it]  2%|‚ñè         | 286/18420 [11:29<12:01:52,  2.39s/it]  2%|‚ñè         | 287/18420 [11:32<12:01:35,  2.39s/it]  2%|‚ñè         | 288/18420 [11:34<12:02:52,  2.39s/it]  2%|‚ñè         | 289/18420 [11:36<11:59:07,  2.38s/it]  2%|‚ñè         | 290/18420 [11:39<12:01:19,  2.39s/it]  2%|‚ñè         | 291/18420 [11:41<12:01:33,  2.39s/it]  2%|‚ñè         | 292/18420 [11:44<12:02:09,  2.39s/it]  2%|‚ñè         | 293/18420 [11:46<12:00:57,  2.39s/it]  2%|‚ñè         | 294/18420 [11:48<12:00:04,  2.38s/it]  2%|‚ñè         | 295/18420 [11:51<12:02:03,  2.39s/it]  2%|‚ñè         | 296/18420 [11:53<12:03:10,  2.39s/it]  2%|‚ñè         | 297/18420 [11:55<11:58:34,  2.38s/it]  2%|‚ñè         | 298/18420 [11:58<11:59:33,  2.38s/it]  2%|‚ñè         | 299/18420 [12:00<12:01:01,  2.39s/it]  2%|‚ñè         | 300/18420 [12:03<12:00:04,  2.38s/it]                                                        2%|‚ñè         | 300/18420 [12:03<12:00:04,  2.38s/it]  2%|‚ñè         | 301/18420 [12:05<11:56:32,  2.37s/it]  2%|‚ñè         | 302/18420 [12:07<11:56:50,  2.37s/it]  2%|‚ñè         | 303/18420 [12:10<12:00:45,  2.39s/it]  2%|‚ñè         | 304/18420 [12:12<12:01:21,  2.39s/it]  2%|‚ñè         | 305/18420 [12:15<11:59:40,  2.38s/it]  2%|‚ñè         | 306/18420 [12:17<12:01:38,  2.39s/it]  2%|‚ñè         | 307/18420 [12:19<12:05:41,  2.40s/it]  2%|‚ñè         | 308/18420 [12:22<12:05:11,  2.40s/it]  2%|‚ñè         | 309/18420 [12:24<12:03:31,  2.40s/it]  2%|‚ñè         | 310/18420 [12:27<12:03:03,  2.40s/it]  2%|‚ñè         | 311/18420 [12:29<12:02:28,  2.39s/it]  2%|‚ñè         | 312/18420 [12:31<12:01:16,  2.39s/it]  2%|‚ñè         | 313/18420 [12:34<11:58:12,  2.38s/it]  2%|‚ñè         | 314/18420 [12:36<11:58:57,  2.38s/it]  2%|‚ñè         | 315/18420 [12:39<12:01:45,  2.39s/it]  2%|‚ñè         | 316/18420 [12:41<12:02:00,  2.39s/it]  2%|‚ñè         | 317/18420 [12:43<12:01:58,  2.39s/it]  2%|‚ñè         | 318/18420 [12:46<12:00:48,  2.39s/it]  2%|‚ñè         | 319/18420 [12:48<12:01:29,  2.39s/it]  2%|‚ñè         | 320/18420 [12:50<12:03:00,  2.40s/it]  2%|‚ñè         | 321/18420 [12:53<11:59:59,  2.39s/it]  2%|‚ñè         | 322/18420 [12:55<11:57:28,  2.38s/it]  2%|‚ñè         | 323/18420 [12:58<12:01:14,  2.39s/it]  2%|‚ñè         | 324/18420 [13:00<12:00:21,  2.39s/it]  2%|‚ñè         | 325/18420 [13:02<11:57:37,  2.38s/it]  2%|‚ñè         | 326/18420 [13:05<12:00:40,  2.39s/it]  2%|‚ñè         | 327/18420 [13:07<12:02:04,  2.39s/it]  2%|‚ñè         | 328/18420 [13:10<12:04:39,  2.40s/it]  2%|‚ñè         | 329/18420 [13:12<12:01:05,  2.39s/it]  2%|‚ñè         | 330/18420 [13:14<12:02:06,  2.40s/it]  2%|‚ñè         | 331/18420 [13:17<12:03:57,  2.40s/it]  2%|‚ñè         | 332/18420 [13:19<12:03:26,  2.40s/it]  2%|‚ñè         | 333/18420 [13:22<12:02:31,  2.40s/it]  2%|‚ñè         | 334/18420 [13:24<12:02:26,  2.40s/it]  2%|‚ñè         | 335/18420 [13:26<12:02:41,  2.40s/it]  2%|‚ñè         | 336/18420 [13:29<12:02:52,  2.40s/it]  2%|‚ñè         | 337/18420 [13:31<11:59:33,  2.39s/it]  2%|‚ñè         | 338/18420 [13:34<11:59:27,  2.39s/it]  2%|‚ñè         | 339/18420 [13:36<12:02:19,  2.40s/it]  2%|‚ñè         | 340/18420 [13:38<12:02:35,  2.40s/it]  2%|‚ñè         | 341/18420 [13:41<12:02:32,  2.40s/it]  2%|‚ñè         | 342/18420 [13:43<12:01:52,  2.40s/it]  2%|‚ñè         | 343/18420 [13:46<12:02:22,  2.40s/it]  2%|‚ñè         | 344/18420 [13:48<12:04:19,  2.40s/it]  2%|‚ñè         | 345/18420 [13:50<12:01:31,  2.40s/it]  2%|‚ñè         | 346/18420 [13:53<12:03:40,  2.40s/it]  2%|‚ñè         | 347/18420 [13:55<12:06:11,  2.41s/it]  2%|‚ñè         | 348/18420 [13:58<12:04:20,  2.40s/it]  2%|‚ñè         | 349/18420 [14:00<12:04:26,  2.41s/it]  2%|‚ñè         | 350/18420 [14:02<12:01:28,  2.40s/it]  2%|‚ñè         | 351/18420 [14:05<12:00:57,  2.39s/it]  2%|‚ñè         | 352/18420 [14:07<12:00:10,  2.39s/it]  2%|‚ñè         | 353/18420 [14:10<11:59:36,  2.39s/it]  2%|‚ñè         | 354/18420 [14:12<12:01:20,  2.40s/it]  2%|‚ñè         | 355/18420 [14:14<12:04:44,  2.41s/it]  2%|‚ñè         | 356/18420 [14:17<12:03:32,  2.40s/it]  2%|‚ñè         | 357/18420 [14:19<12:00:57,  2.39s/it]  2%|‚ñè         | 358/18420 [14:22<12:02:40,  2.40s/it]  2%|‚ñè         | 359/18420 [14:24<12:03:09,  2.40s/it]  2%|‚ñè         | 360/18420 [14:26<12:03:14,  2.40s/it]  2%|‚ñè         | 361/18420 [14:29<12:00:06,  2.39s/it]  2%|‚ñè         | 362/18420 [14:31<12:01:08,  2.40s/it]  2%|‚ñè         | 363/18420 [14:34<12:03:30,  2.40s/it]  2%|‚ñè         | 364/18420 [14:36<12:03:19,  2.40s/it]  2%|‚ñè         | 365/18420 [14:38<12:02:48,  2.40s/it]  2%|‚ñè         | 366/18420 [14:41<12:01:54,  2.40s/it]  2%|‚ñè         | 367/18420 [14:43<12:02:31,  2.40s/it]  2%|‚ñè         | 368/18420 [14:46<12:03:02,  2.40s/it]  2%|‚ñè         | 369/18420 [14:48<12:00:20,  2.39s/it]  2%|‚ñè         | 370/18420 [14:50<12:01:40,  2.40s/it]  2%|‚ñè         | 371/18420 [14:53<12:03:07,  2.40s/it]  2%|‚ñè         | 372/18420 [14:55<12:02:58,  2.40s/it]  2%|‚ñè         | 373/18420 [14:58<12:01:49,  2.40s/it]  2%|‚ñè         | 374/18420 [15:00<12:03:44,  2.41s/it]  2%|‚ñè         | 375/18420 [15:02<12:02:59,  2.40s/it]  2%|‚ñè         | 376/18420 [15:05<12:02:17,  2.40s/it]  2%|‚ñè         | 377/18420 [15:07<11:59:23,  2.39s/it]  2%|‚ñè         | 378/18420 [15:10<12:01:34,  2.40s/it]  2%|‚ñè         | 379/18420 [15:12<12:03:56,  2.41s/it]  2%|‚ñè         | 380/18420 [15:14<12:04:40,  2.41s/it]  2%|‚ñè         | 381/18420 [15:17<12:03:59,  2.41s/it]  2%|‚ñè         | 382/18420 [15:19<12:02:47,  2.40s/it]  2%|‚ñè         | 383/18420 [15:22<12:01:30,  2.40s/it]  2%|‚ñè         | 384/18420 [15:24<12:00:45,  2.40s/it]  2%|‚ñè         | 385/18420 [15:26<11:56:18,  2.38s/it]  2%|‚ñè         | 386/18420 [15:29<11:58:03,  2.39s/it]  2%|‚ñè         | 387/18420 [15:31<12:00:41,  2.40s/it]  2%|‚ñè         | 388/18420 [15:34<12:00:03,  2.40s/it]  2%|‚ñè         | 389/18420 [15:36<12:00:43,  2.40s/it]  2%|‚ñè         | 390/18420 [15:38<12:00:36,  2.40s/it]  2%|‚ñè         | 391/18420 [15:41<12:01:37,  2.40s/it]  2%|‚ñè         | 392/18420 [15:43<12:01:07,  2.40s/it]  2%|‚ñè         | 393/18420 [15:46<11:58:19,  2.39s/it]  2%|‚ñè         | 394/18420 [15:48<12:01:19,  2.40s/it]{'loss': 5.1924, 'grad_norm': 1519470.125, 'learning_rate': 4.9728555917481e-05, 'epoch': 0.05}
{'loss': 2.5931, 'grad_norm': 1494499.625, 'learning_rate': 4.9457111834962e-05, 'epoch': 0.11}
{'loss': 2.2775, 'grad_norm': 1078247.375, 'learning_rate': 4.9185667752443e-05, 'epoch': 0.16}
Traceback (most recent call last):
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 187, in <module>
    main()
  File "/home/nlplab/ssd1/gyop/research/TDRLMoE/baseline_switchT.py", line 152, in main
    trainer.train()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/transformers/trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 101, in parallel_apply
    thread.join()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/nlplab/anaconda3/envs/gyop/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
[1;34mwandb[0m: üöÄ View run [33mswitch-samsum-base16-run[0m at: [34mhttps://wandb.ai/isnlp_lab/samsum-google-switch-base-16/runs/z2a6urkw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250221_033334-z2a6urkw/logs[0m
